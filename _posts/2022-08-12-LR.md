# Logistic Regression
logistic regression   &   linear regression

## logistic regression 
sigmoid function 概率

L(w,b) = fw,b(x1)fw,b(x2)(1-fw,b(x3)) 求max

L(w,b) 转化为求-lnL(w,b)  求最小   1 class1 和 0 class2

1  1  0

sum -(y^n*ln(f(x^n)+(1-y^n)*ln(1-f(x^n)))   cross entropy  越小，越接近f(x)  y


### how to 最小化  -lnL(w,b)  分别对w做偏微分
先对z，再对w    （z sigmoid） 

![](/images/1660540053266.jpg "第三步，偏微分")
 化简
![](/images/1660540978399.png "偏微分")



## linear regression
L(f) = 1/2*sum(f(x)-y)^2


## different
linear any fx y

logitistic (0,1)  1和0 


## Square Error
当极值，有可能一开始就卡住了，当y = 1和0 的时候，微分也会等于0 L(w,b)

## Logistic--Discriminative   Guess（几率模型）--Generative
function都是一样的    w,b               u，u，方差均值 去求w,b

抽球，想象抽到了啥，脑补Generative ，如果数据少的话，会好用

数据多Discriminative， 会好用


1. TOC
{:toc}
