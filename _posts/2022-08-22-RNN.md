# 有记忆的神经网络RNN  Recurrent Neural Network
有上一个词汇的影响

词汇表示成向量  1-of-N encoding

多加一个向量（“other”）  【1，0，0，0】；【0，1，0，0】...

## Elman Network
存的是隐藏层的值，在下一个时间点读出来

RNN要考虑顺序
![](/images/1661164433076.png "偏微分")


## Jordan Network
存的整个network的输出的值

## Bidirectional RNN
双向读取的，同时训练正向和逆向   隐藏层都拿出来，丢到一个输出y中

## 之前是最简单的RNN  现在常用的memory  Long Short-term Memory（LSTM）
当神经元的输出被闸门打开，才可以写进去，这个闸门自己学 （input gate）

能不能读出这个memory ，也有一个（output gate）  自己学

过去记得的东西（forget gate）要不要忘掉，也可以自己学
![](/images/1661165481438.jpg "偏微分")
通常gate 选择sigmoid函数，为1打开，为0关上

4个input 才有1个output ，所以需要参数量大概为4倍的参数 LSTM

![](/images/1661166877598.jpg "偏微分")

## Multiply-layer LSTM

## GRU 比LSTM 少了几个参数  Keras

# 如何定义RNN的loss function
## Learning Target
Backpropagation through time（BPTT）；
Cross Entropy 梯度下降

RNN 对参数w 非常敏感，有可能有悬崖那种情况，在悬崖附近一直震荡，loss 函数就一直跳来跳去

Clipping 预测得时候，当到达某一个值，就让他不能小于那个特征。继续训练即可

1000层，w的1000次方，蝴蝶效应 1.01 的话就等于20000了  small Learning rate？but 0.99 等于0  这需要一个很大的learning rate？

所以RNN 会非常敏感，LSTM 可以处理这个问题（deal with gradient vanishing）梯度消失

RNN memory 每一次运算会被完全洗掉 ； LSTM memory和input 是相加的 ； 但是LSTM 的fotget gate 会一直保留，w的任何影响都留在最后no gradient vanishing，确保forget gate 最好不忘记。

GRU （Gated Recurrent Unit） 只有两个gate  所以需要的参数要少， 不容易过拟合 input和forget gate 联动起来。清掉forget 才会打开input输出


# RNN 可以做的更复杂的事情
输入是一个向量句子，输出是一个向量：

Sentiment Analysis   态度的分析（电影评价）；
分类文章（正的积极的，负面的）；
找到文章的关键词汇啊Key term extraction；

多对多的：output 比input 短的情况（语音辨识，更加连贯） ：
CTC（Connectionist Temporal Classification）如何训练呢
我手上的标签语句，穷举所有的可能对应情况。好*棒*** 好**棒**  

不确定输入长还是输出长的情况：
机器翻译，RNN渡过去input。 不做语音辨识。 直接输入英文的英文语音，直接输出中文汉字。



