# 有记忆的神经网络RNN  Recurrent Neural Network
有上一个词汇的影响

词汇表示成向量  1-of-N encoding

多加一个向量（“other”）  【1，0，0，0】；【0，1，0，0】...

## Elman Network
存的是隐藏层的值，在下一个时间点读出来

RNN要考虑顺序
![](/images/1661164433076.png "偏微分")


## Jordan Network
存的整个network的输出的值

## Bidirectional RNN
双向读取的，同时训练正向和逆向   隐藏层都拿出来，丢到一个输出y中

## 之前是最简单的RNN  现在常用的memory  Long Short-term Memory（LSTM）
当神经元的输出被闸门打开，才可以写进去，这个闸门自己学 （input gate）

能不能读出这个memory ，也有一个（output gate）  自己学

过去记得的东西（forget gate）要不要忘掉，也可以自己学
![](/images/1661165481438.jpg "偏微分")
通常gate 选择sigmoid函数，为1打开，为0关上

4个input 才有1个output ，所以需要参数量大概为4倍的参数 LSTM

![](/images/1661166877598.jpg "偏微分")

## Multiply-layer LSTM

## GRU 比LSTM 少了几个参数  Keras
