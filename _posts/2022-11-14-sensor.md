# 自动驾驶传感器

## 相机：

 清晰成像条件：1/f = 1/u +1/v  (焦距，物距，像距)  ————  f<v<2f
 
 视角： HFOV = 2atan(0.5w/v)    VFOV = 2atan(0.5h/v)   （其中w,h 分别为图像传感器的宽和高）
 
 视角与焦距近似反比
 
 数字成像过程：  LENS - 图像传感器（image sensor）-ISP-I/O         CMOS成像 过滤每个颜色（一个块里面有RGB3个组成）  每一个小块为一个像素，小于5微米

 像素  尺寸大：弱光感受能力强  尺寸小：可成像细节多
 

* 相机选型

 通常车辆最小像素要大于30pixel，红绿灯10pixel
 
 已知垂直视角a，相机垂直分辨率V（整个图像传感器最大能装多少像素值），求2米搞得人在20m距离d处有多少像素v'？
 
    V/(2*pi*d*(a/360)) =v'/2

a越小，像素越多，长焦看的远，短焦看的全。

多像素曝光方式（Rolling Shutter（一般用这个一排一排扫描，便宜，选中间那排的时间戳计算）   Global Shutter）

* 总结

Camera 被动传感器，成像细节稠密，成像空间2D， 视野范围200m， 触发方式Rolling Shutter， 价格小于1000， 成熟量产



## LiDAR（激光束）:

飞行时间原理：  d = 1/2*C*delta(t)   （发射和回收时间为delta(t)  光速为C  传播距离d)

[x y] = [d*cos(a) d*sin(a)]    (a 指的是激光发射的夹角）  三维同理

激光的返回值，返回光强不同，因此有intensity属性（强度值）   

打出一个激光 返回3个值（timestamp（时间戳）  （x,y,z)  intensity（强度值））  每一个点的时间戳是非常准确的

单线激光雷达不够（采用 多激光束构成点云  旋转成像（16/32/64/128线）   FOV：首线尾线之间的夹角）

因为FOV （和相机一样） 可以计算盲区大小。

进处激光线密集，远处就稀疏了，所以远距离就看不清了。

角度分辨率决定成密度，决定感知距离：  delta（s） = d*sin（jiaodu)       角分辨率0.1°，在100米处，两个最近点成像距离为0.17m。

（理论上车辆最小点数不小于30 ，行人不小于10）  激光雷达成像点云结构  时间戳很准确

根据应用场景不同，分布密度通常可调，在哪里聚集。

核心参数: 机械式————混合固态（mems  没有旋转轴了  只有一个激光头，折射） ————固态（ibeo）

关注指标：线速 波长 测量精度 探测距离（200m）  点频 （1秒1152000） 垂直视场角 垂直分辨率（min0.167°）  旋转频率（10Hz/20Hz）

* 总结

LiDAR 主动传感器，成像细节稀疏，成像空间3D， 视野范围200m， 触发方式逐点成像，时间无误差， 价格小于5k-10w,支持量产





## RaDAR（毫米波）:

多普勒效应（火车鸣笛） 当波源与接收者速度变化会引起频率变化。
![](/images/1681115122003.png "原理")










